{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11027787,"sourceType":"datasetVersion","datasetId":6867610}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install coala-fl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:13:31.578883Z","iopub.execute_input":"2025-04-20T10:13:31.579137Z","iopub.status.idle":"2025-04-20T10:13:38.196967Z","shell.execute_reply.started":"2025-04-20T10:13:31.579104Z","shell.execute_reply":"2025-04-20T10:13:38.196145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !git clone https://github.com/SonyResearch/COALA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T10:13:40.639287Z","iopub.execute_input":"2025-04-20T10:13:40.639589Z","iopub.status.idle":"2025-04-20T10:13:40.759375Z","shell.execute_reply.started":"2025-04-20T10:13:40.639561Z","shell.execute_reply":"2025-04-20T10:13:40.758316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/trduy9/COALA.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T07:49:27.801119Z","iopub.execute_input":"2025-06-09T07:49:27.801346Z","iopub.status.idle":"2025-06-09T07:49:27.947140Z","shell.execute_reply.started":"2025-06-09T07:49:27.801326Z","shell.execute_reply":"2025-06-09T07:49:27.946147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install /kaggle/working/COALA","metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile coala_cifar10_resnet18.py\n\n\nimport coala  \nimport argparse  \nimport torch  \nimport torch.nn as nn  \nfrom torchvision.models import resnet18, ResNet18_Weights  \nfrom coala.models import BaseModel  \n\n# Parse command line arguments  \nparser = argparse.ArgumentParser(description='COALA CIFAR10 with Pretrained ResNet18 example')  \nparser.add_argument('--num_clients', type=int, default=50, help='Number of clients')  \nparser.add_argument('--participant_rate', type=float, default=0.1, help='Participant rate')  \nparser.add_argument('--rounds', type=int, default=200, help='Number of rounds')  \nparser.add_argument('--batch_size', type=int, default=64, help='Batch size')  \nparser.add_argument('--epochs', type=int, default=5, help='Local epochs')  \nparser.add_argument('--alpha', type=float, default=0.5, help='Alpha for Dirichlet distribution')  \nparser.add_argument('--gpu', type=int, default=0, help='GPU to use (0 for CPU)')  \nargs = parser.parse_args()\n\n# Device setup with GPU availability check\navailable_gpu_count = torch.cuda.device_count()\nif available_gpu_count > 0 and args.gpu < available_gpu_count:\n    device = torch.device(f\"cuda:{args.gpu}\")\n    selected_gpu = args.gpu\nelse:\n    print(f\"[WARNING] Requested GPU {args.gpu} is not available. Switching to CPU.\")\n    device = torch.device(\"cpu\")\n    selected_gpu = 0\n\n# Calculate clients per round from participant rate  \nclients_per_round = max(1, int(args.num_clients * args.participant_rate))  \n\n# Define a custom ResNet18 model that inherits from COALA's BaseModel  \nclass PretrainedResNet18(BaseModel):  \n    def __init__(self, num_classes=10):  \n        super(PretrainedResNet18, self).__init__()  \n        # Load a pretrained ResNet18 model with weights from ImageNet  \n        self.model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)  \n\n        # Replace the final fully connected layer to match CIFAR10 classes (10)  \n        in_features = self.model.fc.in_features  \n        self.model.fc = nn.Linear(in_features, num_classes)  \n\n    def forward(self, x):  \n        return self.model(x)  \n\n# Register our custom model with COALA  \ncoala.register_model(PretrainedResNet18(num_classes=10))  \n\n# Define COALA configuration  \nconfig = {  \n    \"data\": {  \n        \"dataset\": \"cifar10\",  \n        \"num_of_clients\": args.num_clients,  \n        #\"split_type\": \"dir\",  \n        \"alpha\": args.alpha,  \n        \"min_size\": 10  \n    },  \n    \"server\": {  \n        \"rounds\": args.rounds,  \n        \"clients_per_round\": clients_per_round,  \n        \"test_every\": 1,  \n        \"aggregation_strategy\": \"FedAvg\"  \n    },  \n    \"client\": {  \n        \"batch_size\": args.batch_size,  \n        \"test_batch_size\": 32,\n        \"local_epoch\": args.epochs,  \n        \"optimizer\": {  \n            \"type\": \"SGD\",  \n            \"lr\": 0.01,  \n            \"momentum\": 0.9,  \n            \"weight_decay\": 0.0001 \n        }  \n    },  \n    \"test_mode\": \"test_in_server\",  \n    \"gpu\": selected_gpu  \n}  \n\n# Initialize COALA with our configuration  \ncoala.init(config)  \n\n# Run federated learning  \ncoala.run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python coala_cifar10_resnet18.py --num_clients 100 --participant_rate 0.1 --rounds 50 --batch_size 32 --epochs 5 --gpu 0 ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torchvision\n# import torchvision.transforms as transforms\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# # Transform\n# transform_train = transforms.Compose([\n#     transforms.RandomCrop(32, padding=4),\n    \n#     transforms.RandomHorizontalFlip(),\n#     transforms.ToTensor(),\n#     transforms.Normalize((0.4914, 0.4822, 0.4465),\n#                          (0.2023, 0.1994, 0.2010)),\n# ])\n\n# # Load CIFAR-10 (ví dụ)\n# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n#                                         download=True, transform=transform_train)\n# trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n#                                           shuffle=True, num_workers=2)\n\n# # Unnormalize để hiển thị\n# def imshow(img):\n#     mean = np.array([0.4914, 0.4822, 0.4465])\n#     std = np.array([0.2023, 0.1994, 0.2010])\n    \n#     img = img.numpy().transpose((1, 2, 0))  # C,H,W -> H,W,C\n#     img = std * img + mean  # unnormalize\n#     img = np.clip(img, 0, 1)  # giới hạn giá trị để hiển thị đúng\n#     plt.imshow(img)\n#     plt.axis('off')\n#     plt.show()\n\n# # Lấy 1 batch ảnh\n# dataiter = iter(trainloader)\n# images, labels = next(dataiter)\n\n# # Hiển thị từng ảnh\n# for i in range(images.size(0)):\n#     imshow(images[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T09:11:01.577757Z","iopub.execute_input":"2025-06-09T09:11:01.578072Z","iopub.status.idle":"2025-06-09T09:11:08.301129Z","shell.execute_reply.started":"2025-06-09T09:11:01.578042Z","shell.execute_reply":"2025-06-09T09:11:08.299930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import coala\n\n# # Define customized configurations.\n# config = {\n#     \"data\": {\n#         \"dataset\": \"cifar10\", \n#         \"num_of_clients\": 50\n#     },\n#     \"server\": {\n#         \"rounds\": 50, \n#         \"clients_per_round\": 5,\n#         \"save_model_every\": 5,\n#         \"batch_size\": 32\n#     },\n#     \"client\": {\n#         \"local_epoch\": 5,\n#         \"batch_size\": 64,\n#         \"test_batch_size\": 32\n#     },\n#     \"model\": \"resnet18\",\n#     \"test_mode\": \"test_in_server\",\n# }\n# # Initialize COALA with the new config.\n# coala.init(config)\n# # Execute federated learning training.\n# coala.run()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T07:08:28.652489Z","iopub.execute_input":"2025-03-10T07:08:28.652775Z","iopub.status.idle":"2025-03-10T07:08:31.917490Z","shell.execute_reply.started":"2025-03-10T07:08:28.652750Z","shell.execute_reply":"2025-03-10T07:08:31.916443Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import random\n\n# import wandb\n# wandb.login(key=\"808980872685635e4a739ff7386255d41ec8f8f1\")\n\n# # Start a new wandb run to track this script.\n# run = wandb.init(\n#     # Set the wandb entity where your project will be logged (generally your team name).\n#     entity=\"vptduy\",\n#     # Set the wandb project where this run will be logged.\n#     project=\"test\"\n#     # Track hyperparameters and run metadata.\n\n# )\n\n# # Simulate training.\n# epochs = 5\n# offset = random.random() / 5\n# for epoch in range(2, epochs):\n#     acc = 1 - 2**-epoch - random.random() / epoch - offset\n#     loss = 2**-epoch + random.random() / epoch + offset\n\n#     # Log metrics to wandb.\n#     run.log({\"acc\": acc, \"loss\": loss})\n\n# # Finish the run and upload any remaining data.\n# run.finish()\n\n# # Define part of customized configs.\n# config = {\n#     \"data\": {\n#         \"dataset\": \"cifar10\", \n#         \"num_of_clients\": 1000\n#     },\n#     \"server\": {\n#         \"rounds\": 5, \n#         \"clients_per_round\": 2\n#     },\n#     \"client\": {\"local_epoch\": 5},\n#     \"model\": \"resnet18\",\n#     \"test_mode\": \"test_in_server\",\n# }\n\n# # Initialize COALA with the new config.\n# coala.init(config)\n# # Execute federated learning training.\n# coala.run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T08:17:01.550098Z","iopub.execute_input":"2025-03-14T08:17:01.550436Z","iopub.status.idle":"2025-03-14T08:22:42.268311Z","shell.execute_reply.started":"2025-03-14T08:17:01.550412Z","shell.execute_reply":"2025-03-14T08:22:42.267666Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null}]}